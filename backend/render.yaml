services:
  - type: web
    name: llama-backend
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: PYTHON_VERSION
        value: 3.11.0
      - key: PORT
        value: 8000
      - key: OLLAMA_API_URL
        value: http://ollama:11434/api/chat
      - key: SECRET_KEY
        sync: false
      - key: OLLAMA_TIMEOUT
        value: "30.0"
      - key: OLLAMA_MAX_RETRIES
        value: "3"
      - key: MAX_CONNECTIONS
        value: "10"
      - key: DB_TIMEOUT
        value: "30.0"
      - key: RATE_LIMIT_REQUESTS
        value: "10"
      - key: RATE_LIMIT_PERIOD
        value: "60"
      - key: MAX_REQUEST_SIZE
        value: "104857600"
      - key: MINIGPT4_URL
        value: "http://localhost:7860"
    serviceDiscovery:
      - name: ollama
        type: web

  - type: web
    name: ollama
    env: docker
    dockerfilePath: ./Dockerfile.ollama
    envVars:
      - key: OLLAMA_HOST
        value: 0.0.0.0
      - key: OLLAMA_PORT
        value: 11434
      - key: OLLAMA_DEBUG
        value: "1"
      - key: OLLAMA_ORIGINS
        value: "*"
      - key: OLLAMA_MODELS
        value: "mistral:latest,bakllava:latest,gemma:7b"
    healthCheckPath: /api/tags
    healthCheckTimeout: 300
    autoDeploy: true
    numInstances: 1
    plan: free
    scaling:
      minInstances: 1
      maxInstances: 1 